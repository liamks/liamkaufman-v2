<!DOCTYPE html>
<html lang="en">

<head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script>
  <title>
  Scraping Web Pages with jQuery, Node.js and Jsdom · Liam Kaufman
</title>
  <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="color-scheme" content="light dark">




<meta name="author" content="Liam Kaufman">
<meta name="description" content="
I always found it odd that accessing DOM elements with Ruby, or Python, wasn&#39;t as easy as it was with jQuery. Many HTML parsing libraries employ Simple API for XML (SAX) that can handle extremely large XML documents, but is cumbersome and adds complexity. Other parsing libraries use XML Path Language (XPath), which is conceptually simpler than SAX, but still more of an effort than jQuery. I was pleasantly surprised to discover that it&#39;s possible to use jQuery to parse web pages with Node.js. This is accomplished by using jsdom, &#34;a javascript implementation of the W3C DOM&#34;.
">
<meta name="keywords" content="blog,developer,personal">



  <meta name="twitter:card" content="summary">
  <meta name="twitter:title" content="Scraping Web Pages with jQuery, Node.js and Jsdom">
  <meta name="twitter:description" content="I always found it odd that accessing DOM elements with Ruby, or Python, wasn&#39;t as easy as it was with jQuery. Many HTML parsing libraries employ Simple API for XML (SAX) that can handle extremely large XML documents, but is cumbersome and adds complexity. Other parsing libraries use XML Path Language (XPath), which is conceptually simpler than SAX, but still more of an effort than jQuery. I was pleasantly surprised to discover that it&#39;s possible to use jQuery to parse web pages with Node.js. This is accomplished by using jsdom, &#34;a javascript implementation of the W3C DOM&#34;.">

<meta property="og:url" content="http://localhost:1313/blog/2012/03/08/scraping-web-pages-with-jquery-nodejs-and-jsdom/">
  <meta property="og:site_name" content="Liam Kaufman">
  <meta property="og:title" content="Scraping Web Pages with jQuery, Node.js and Jsdom">
  <meta property="og:description" content="I always found it odd that accessing DOM elements with Ruby, or Python, wasn&#39;t as easy as it was with jQuery. Many HTML parsing libraries employ Simple API for XML (SAX) that can handle extremely large XML documents, but is cumbersome and adds complexity. Other parsing libraries use XML Path Language (XPath), which is conceptually simpler than SAX, but still more of an effort than jQuery. I was pleasantly surprised to discover that it&#39;s possible to use jQuery to parse web pages with Node.js. This is accomplished by using jsdom, &#34;a javascript implementation of the W3C DOM&#34;.">
  <meta property="og:locale" content="en">
  <meta property="og:type" content="article">
    <meta property="article:section" content="blog">
    <meta property="article:published_time" content="2012-03-08T00:00:00+00:00">
    <meta property="article:modified_time" content="2012-03-08T00:00:00+00:00">




<link rel="canonical" href="http://localhost:1313/blog/2012/03/08/scraping-web-pages-with-jquery-nodejs-and-jsdom/">


<link rel="preload" href="/fonts/fa-brands-400.woff2" as="font" type="font/woff2" crossorigin>
<link rel="preload" href="/fonts/fa-regular-400.woff2" as="font" type="font/woff2" crossorigin>
<link rel="preload" href="/fonts/fa-solid-900.woff2" as="font" type="font/woff2" crossorigin>


  
  
  <link rel="stylesheet" href="/css/coder.css" media="screen">








 




<link rel="icon" type="image/svg+xml" href="/img/favicon.svg" sizes="any">
<link rel="icon" type="image/png" href="/img/favicon-32x32.png" sizes="32x32">
<link rel="icon" type="image/png" href="/img/favicon-16x16.png" sizes="16x16">

<link rel="apple-touch-icon" href="/images/apple-touch-icon.png">
<link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon.png">



<link rel="mask-icon" href="/images/safari-pinned-tab.svg" color="#5bbad5">









</head>




<body class="preload-transitions colorscheme-light">
  
<div class="float-container">
    <a id="dark-mode-toggle" class="colorscheme-toggle">
        <i class="fa-solid fa-adjust fa-fw" aria-hidden="true"></i>
    </a>
</div>


  <main class="wrapper">
    <nav class="navigation">
  <section class="container">
    
    <a class="navigation-title" href="http://localhost:1313/">
      Liam Kaufman
    </a>
    
    
      <input type="checkbox" id="menu-toggle" />
      <label class="menu-button float-right" for="menu-toggle">
        <i class="fa-solid fa-bars fa-fw" aria-hidden="true"></i>
      </label>
      <ul class="navigation-list">
        
          
            <li class="navigation-item">
              <a class="navigation-link " href="/posts/">Blog</a>
            </li>
          
            <li class="navigation-item">
              <a class="navigation-link " href="/about/">About</a>
            </li>
          
        
        
      </ul>
    
  </section>
</nav>


    <div class="content">
      
  <section class="container post">
    <article>
      <header>
        <div class="post-title">
          <h1 class="title">
            <a class="title-link" href="http://localhost:1313/blog/2012/03/08/scraping-web-pages-with-jquery-nodejs-and-jsdom/">
              Scraping Web Pages with jQuery, Node.js and Jsdom
            </a>
          </h1>
        </div>
        <div class="post-meta">
          <div class="date">
            <span class="posted-on">
              <i class="fa-solid fa-calendar" aria-hidden="true"></i>
              <time datetime="2012-03-08T00:00:00Z">
                March 8, 2012
              </time>
            </span>
            <span class="reading-time">
              <i class="fa-solid fa-clock" aria-hidden="true"></i>
              3-minute read
            </span>
          </div>
          
          
          
        </div>
      </header>

      <div class="post-content">
        
        
<p>
I always found it odd that accessing DOM elements with Ruby, or Python, wasn't as easy as it was with jQuery. Many HTML parsing libraries employ Simple API for XML (SAX) that can handle extremely large XML documents, but is cumbersome and adds complexity. Other parsing libraries use XML Path Language (XPath), which is conceptually simpler than SAX, but still more of an effort than jQuery. I was pleasantly surprised to discover that it's possible to use jQuery to parse web pages with Node.js. This is accomplished by using <a href="https://github.com/tmpvar/jsdom">jsdom, "a javascript implementation of the W3C DOM"</a>.
</p>

<h2>jQuery and jsdom</h2>
<p>
Using jsdom you can specify a local file, or url, and jsdom will return the <code>window</code> object for that document. Additionally, JavaScript can be inserted into the document; in our case we're inserting the jQuery library. In the example below all the links from the Hacker News front page are logged to the console.
</p>

```javascript Scraping Links From Hacker News
var jsdom  = require('jsdom');
var fs     = require('fs');
var jquery = fs.readFileSync("./jquery-1.7.1.min.js").toString();

jsdom.env({
  html: 'http://news.ycombinator.com/',
  src: [
    jquery
  ],
  done: function(errors, window) {
    var $ = window.$;
    $('a').each(function(){
      console.log( $(this).attr('href') );
    });
  }
});
```

<h2>Making Scraping More Robust</h2>
<p>
  Unfortunately there are few common bugs that I ran into when scraping content with jQuery and jsdom. Specifically there are two issues, that aren't necessarily specific to jsdom, that are worth watching out for. 
</p>

<h3>jQuery Return Values</h3>
<p>
  The first issue are return values from jQuery function calls. Extra attention has to be paid to return values. Applying a method to <code>undefined</code> will crash a program, a problem that can be especially apparent in DOM parsing. Consider the example below:
</p>

```javascript
$($('a')[7]).attr('href').split('/')
```
<p>
  If there are 8, or more, links on a page the 8th link will be returned and its href attribute will be split into an array. However, if there are less than 8 the <code>attr('href')</code> will return <code>undefined</code> and calling <code>split()</code> on it will crash the program. Since HTML pages aren't as structured as API responses, its important not to assume too much and always check return values.
</p>

<h3>Web Page Errors</h3>
<p>
  It's entirely possible that the url passed to jsdom returns an error. If the error is temporary, your scraper might miss out on important information. This issue can be mitigated by recursively retrying the url, like the example below:
</p>

```javascript Managing Errors
var getLinks = function(retries){

  if(retries === 3){
    return;
  }else if (retries === undefined){
    retries = 0;
  }

  jsdom.env({
    html: 'http://news.ycombinator.com/',
    src: [
      jquery
    ],
    done: function(errors, window) {

      if(errors){
        return getLinks(retries + 1);
      }

      var $ = window.$;
      $('a').each(function(){
        console.log( $(this).attr('href') );
      });
    }
  });
}
```

<p>
With the above approach, if errors are encountered <code>getLinks</code> will be called recursively with a larger <code>retries</code> value. On the 3rd retry the function will return. If you wanted to go further you could wrap the recursive call in <code>setTimeout</code> to ensure that the recursive web request was not made immediately after the error was encountered.
</p>

<h2>Conclusions</h2>
<p>
  Parsing web pages with jQuery on the server is a much more natural experience for developers already accustomed to using jQuery in the client. However, prior to scraping it's worth checking if the site 1) allows scraping and 2) does not already have an API. Consuming a JSON API would be even easy than scraping and parsing!
</p>
      </div>


      <footer>
        <div id="disqus_thread"></div>
<script>
  window.disqus_config = function () {
    this.page.identifier = '\/blog\/2012\/03\/08\/scraping-web-pages-with-jquery-nodejs-and-jsdom\/';
    
    this.page.url = '\/blog\/2012\/03\/08\/scraping-web-pages-with-jquery-nodejs-and-jsdom\/';
    };
    (function() {
        if (["localhost", "127.0.0.1"].indexOf(window.location.hostname) != -1) {
            document.getElementById('disqus_thread').innerHTML = 'Disqus comments not available by default when the website is previewed locally.';
            return;
        }
        var d = document, s = d.createElement('script'); s.async = true;
        s.src = '//' + "liamkaufman" + '.disqus.com/embed.js';
        s.setAttribute('data-timestamp', +new Date());
        (d.head || d.body).appendChild(s);
    })();
    
    document.addEventListener('themeChanged', function (e) { 
        if (document.readyState == 'complete') {
          DISQUS.reset({ reload: true, config: disqus_config });
        }
    });
</script>
      </footer>
    </article>

    
  </section>

    </div>

    <footer class="footer">
  <section class="container">
    ©
    
      2015 -
    
    2025
     Liam Kaufman 
    ·
    
    Powered by <a href="https://gohugo.io/" target="_blank" rel="noopener">Hugo</a> & <a href="https://github.com/luizdepra/hugo-coder/" target="_blank" rel="noopener">Coder</a>.
    
  </section>
</footer>

  </main>

  

  
  
  <script src="/js/coder.js"></script>
  

  

  


  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  
</body>
</html>

<!DOCTYPE html>
<html lang="en">

<head>
  <title>
  FDA Committee meeting for generative AI-enabled mental health medical devices · Liam Kaufman
</title>
  <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="color-scheme" content="light dark">




<meta name="author" content="Liam Kaufman">
<meta name="description" content="Today the FDA held their “Digital Health Advisory Committee Meeting” to get feedback from industry experts (payers, providers, startups, etc), academics and clinicians on “Generative Artificial Intelligence-Enabled Mental Health Medical Devices.” There were a few clear themes across the sessions:

There is a large unmet need for mental health services and therapy. This is caused by many issues: too few clinicians, lack of financial resources to seek help or access to care.
Generative AI has a huge potential to fill the unmet need and democratise mental health care.
AI based mental health tools should be regulated with a risk-based approach like medical devices in general.
There should exist a taxonomy, like with self-driving cars (e.g. level 1 to level 5), for AI used in mental health care.
Many mainstream LLMs are already being used for therapy, but have not been finetuned or rigorously tested for that use case.
There’s a clear need to monitor AI models once they’ve been deployed to track model drift, adverse events, suicidality, etc. Based on the questions at the event it wasn’t clear who will eventually be doing that monitoring, but my guess is that the vendor will have to take this on.
LLM-based mental health tools or digital therapeutics show much greater engagement than traditional digital health tools of pre-2022.
Vendors should make it clear to users that they are talking to a bot - that wasn’t always clear with existing models.

What surprised me the most was the lack of rigor when evaluating these tools. In some cases these tools weren’t evaluated and when they were evaluated it was retrospectively. Even when they were evaluated prospectively the primary endpoint was the PHQ-9. Several speakers called this out and indicated that the PHQ-9 is not the best endpoint for evaluating efficacy.">
<meta name="keywords" content="blog,developer,personal">



  <meta name="twitter:card" content="summary">
  <meta name="twitter:title" content="FDA Committee meeting for generative AI-enabled mental health medical devices">
  <meta name="twitter:description" content="Today the FDA held their “Digital Health Advisory Committee Meeting” to get feedback from industry experts (payers, providers, startups, etc), academics and clinicians on “Generative Artificial Intelligence-Enabled Mental Health Medical Devices.” There were a few clear themes across the sessions:
There is a large unmet need for mental health services and therapy. This is caused by many issues: too few clinicians, lack of financial resources to seek help or access to care. Generative AI has a huge potential to fill the unmet need and democratise mental health care. AI based mental health tools should be regulated with a risk-based approach like medical devices in general. There should exist a taxonomy, like with self-driving cars (e.g. level 1 to level 5), for AI used in mental health care. Many mainstream LLMs are already being used for therapy, but have not been finetuned or rigorously tested for that use case. There’s a clear need to monitor AI models once they’ve been deployed to track model drift, adverse events, suicidality, etc. Based on the questions at the event it wasn’t clear who will eventually be doing that monitoring, but my guess is that the vendor will have to take this on. LLM-based mental health tools or digital therapeutics show much greater engagement than traditional digital health tools of pre-2022. Vendors should make it clear to users that they are talking to a bot - that wasn’t always clear with existing models. What surprised me the most was the lack of rigor when evaluating these tools. In some cases these tools weren’t evaluated and when they were evaluated it was retrospectively. Even when they were evaluated prospectively the primary endpoint was the PHQ-9. Several speakers called this out and indicated that the PHQ-9 is not the best endpoint for evaluating efficacy.">

<meta property="og:url" content="https://liamkaufman.com/posts/fda-generative-ai-enabled-mental-health-devices/">
  <meta property="og:site_name" content="Liam Kaufman">
  <meta property="og:title" content="FDA Committee meeting for generative AI-enabled mental health medical devices">
  <meta property="og:description" content="Today the FDA held their “Digital Health Advisory Committee Meeting” to get feedback from industry experts (payers, providers, startups, etc), academics and clinicians on “Generative Artificial Intelligence-Enabled Mental Health Medical Devices.” There were a few clear themes across the sessions:
There is a large unmet need for mental health services and therapy. This is caused by many issues: too few clinicians, lack of financial resources to seek help or access to care. Generative AI has a huge potential to fill the unmet need and democratise mental health care. AI based mental health tools should be regulated with a risk-based approach like medical devices in general. There should exist a taxonomy, like with self-driving cars (e.g. level 1 to level 5), for AI used in mental health care. Many mainstream LLMs are already being used for therapy, but have not been finetuned or rigorously tested for that use case. There’s a clear need to monitor AI models once they’ve been deployed to track model drift, adverse events, suicidality, etc. Based on the questions at the event it wasn’t clear who will eventually be doing that monitoring, but my guess is that the vendor will have to take this on. LLM-based mental health tools or digital therapeutics show much greater engagement than traditional digital health tools of pre-2022. Vendors should make it clear to users that they are talking to a bot - that wasn’t always clear with existing models. What surprised me the most was the lack of rigor when evaluating these tools. In some cases these tools weren’t evaluated and when they were evaluated it was retrospectively. Even when they were evaluated prospectively the primary endpoint was the PHQ-9. Several speakers called this out and indicated that the PHQ-9 is not the best endpoint for evaluating efficacy.">
  <meta property="og:locale" content="en">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2025-11-06T21:27:30-05:00">
    <meta property="article:modified_time" content="2025-11-06T21:27:30-05:00">




<link rel="canonical" href="https://liamkaufman.com/posts/fda-generative-ai-enabled-mental-health-devices/">


<link rel="preload" href="/fonts/fa-brands-400.woff2" as="font" type="font/woff2" crossorigin>
<link rel="preload" href="/fonts/fa-regular-400.woff2" as="font" type="font/woff2" crossorigin>
<link rel="preload" href="/fonts/fa-solid-900.woff2" as="font" type="font/woff2" crossorigin>


  
  
  <link rel="stylesheet" href="/css/coder.min.4b392a85107b91dbdabc528edf014a6ab1a30cd44cafcd5325c8efe796794fca.css" integrity="sha256-SzkqhRB7kdvavFKO3wFKarGjDNRMr81TJcjv55Z5T8o=" crossorigin="anonymous" media="screen" />








 




<link rel="icon" type="image/svg+xml" href="/img/favicon.svg" sizes="any">
<link rel="icon" type="image/png" href="/img/favicon-32x32.png" sizes="32x32">
<link rel="icon" type="image/png" href="/img/favicon-16x16.png" sizes="16x16">

<link rel="apple-touch-icon" href="/images/apple-touch-icon.png">
<link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon.png">



<link rel="mask-icon" href="/images/safari-pinned-tab.svg" color="#5bbad5">









</head>




<body class="preload-transitions colorscheme-light">
  
<div class="float-container">
    <a id="dark-mode-toggle" class="colorscheme-toggle">
        <i class="fa-solid fa-adjust fa-fw" aria-hidden="true"></i>
    </a>
</div>


  <main class="wrapper">
    <nav class="navigation">
  <section class="container">
    
    <a class="navigation-title" href="https://liamkaufman.com/">
      Liam Kaufman
    </a>
    
    
      <input type="checkbox" id="menu-toggle" />
      <label class="menu-button float-right" for="menu-toggle">
        <i class="fa-solid fa-bars fa-fw" aria-hidden="true"></i>
      </label>
      <ul class="navigation-list">
        
          
            <li class="navigation-item">
              <a class="navigation-link " href="/posts/">Blog</a>
            </li>
          
            <li class="navigation-item">
              <a class="navigation-link " href="/about/">About</a>
            </li>
          
        
        
      </ul>
    
  </section>
</nav>


    <div class="content">
      
  <section class="container post">
    <article>
      <header>
        <div class="post-title">
          <h1 class="title">
            <a class="title-link" href="https://liamkaufman.com/posts/fda-generative-ai-enabled-mental-health-devices/">
              FDA Committee meeting for generative AI-enabled mental health medical devices
            </a>
          </h1>
        </div>
        <div class="post-meta">
          <div class="date">
            <span class="posted-on">
              <i class="fa-solid fa-calendar" aria-hidden="true"></i>
              <time datetime="2025-11-06T21:27:30-05:00">
                November 6, 2025
              </time>
            </span>
            <span class="reading-time">
              <i class="fa-solid fa-clock" aria-hidden="true"></i>
              2-minute read
            </span>
          </div>
          
          
          
        </div>
      </header>

      <div class="post-content">
        
        <p>Today the FDA held their “Digital Health Advisory Committee Meeting” to get feedback from industry experts (payers, providers, startups, etc), academics and clinicians on “Generative Artificial Intelligence-Enabled Mental Health Medical Devices.” There were a few clear themes across the sessions:</p>
<ul>
<li>There is a large unmet need for mental health services and therapy. This is caused by many issues: too few clinicians, lack of financial resources to seek help or access to care.</li>
<li>Generative AI has a huge potential to fill the unmet need and democratise mental health care.</li>
<li>AI based mental health tools should be regulated with a risk-based approach like medical devices in general.</li>
<li>There should exist a taxonomy, like with self-driving cars (e.g. level 1 to level 5), for AI used in mental health care.</li>
<li>Many mainstream LLMs are already being used for therapy, but have not been finetuned or rigorously tested for that use case.</li>
<li>There’s a clear need to monitor AI models once they’ve been deployed to track model drift, adverse events, suicidality, etc. Based on the questions at the event it wasn’t clear who will eventually be doing that monitoring, but my guess is that the vendor will have to take this on.</li>
<li>LLM-based mental health tools or digital therapeutics show much greater engagement than traditional digital health tools of pre-2022.</li>
<li>Vendors should make it clear to users that they are talking to a bot - that wasn’t always clear with existing models.</li>
</ul>
<p>What surprised me the most was the lack of rigor when evaluating these tools. In some cases these tools weren’t evaluated and when they were evaluated it was retrospectively. Even when they were evaluated prospectively the primary endpoint was the PHQ-9. Several speakers called this out and indicated that the PHQ-9 is not the best endpoint for evaluating efficacy.</p>
<p>I think there’s a huge opportunity for start-ups and companies to evaluate their models with the same rigor that life science companies do with their therapies. While I understand that models have different considerations, there is still overlap and the rigor is needed for better clinical outcomes, trust and ultimately payer/regulatory buyin.</p>
<p>Event Info: <a href="https://www.fda.gov/advisory-committees/advisory-committee-calendar/november-6-2025-digital-health-advisory-committee-meeting-announcement-11062025"  class="external-link" target="_blank" rel="noopener">https://www.fda.gov/advisory-committees/advisory-committee-calendar/november-6-2025-digital-health-advisory-committee-meeting-announcement-11062025</a></p>
<p><a href="https://www.fda.gov/media/189391/download"  class="external-link" target="_blank" rel="noopener">Meeting Executive Summery</a></p>

      </div>


      <footer>
      </footer>
    </article>

    
  </section>

    </div>

    <footer class="footer">
  <section class="container">
    ©
    
      2015 -
    
    2026
     Liam Kaufman 
    ·
    
    Powered by <a href="https://gohugo.io/" target="_blank" rel="noopener">Hugo</a> & <a href="https://github.com/luizdepra/hugo-coder/" target="_blank" rel="noopener">Coder</a>.
    
  </section>
</footer>

  </main>

  

  
  
  <script src="/js/coder.min.6ae284be93d2d19dad1f02b0039508d9aab3180a12a06dcc71b0b0ef7825a317.js" integrity="sha256-auKEvpPS0Z2tHwKwA5UI2aqzGAoSoG3McbCw73gloxc="></script>
  

  

  


  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  
</body>
</html>
